{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama (Deep Seek, LLama etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform **vector search** with [ChromaDB](https://www.trychroma.com/)<br><br>Steps\n",
    "1. Create Documents: How should this be done for the text book, what makes sense?\n",
    "2. Try new approaches for text splitting\n",
    "2. Adding id for every chunk, so we can **cite** where the LLM bases it's reasoning from\n",
    "3. The book will be stored in PDF format so we will use PyPDFLoader from langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps:\n",
    "1. How should the \n",
    "\n",
    "2. Try different Embedding functions\n",
    "3. Try different query approaches\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Ideas:\n",
    "1. First prompt should load the right exam as context for the LLM (Maybe with naming the pdfs?)\n",
    "2. First prompt also includes Background knowledge from textbook and \"Erwartungshorizont\"\n",
    "3. Precisely define the task for the LLM\n",
    "4. All prompts after should not include the Info again (-> different templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing:\n",
    "1. Use Pytest for Unit Testing\n",
    "2. Different Prompts and general approaches for Prompting\n",
    "3. Different LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to enable access to this LLM via an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages<br>\n",
    "1. fastapi\n",
    "2. uvicorn (for running the api)\n",
    "3. requests (for sending requests)\n",
    "\n",
    "How does it work with access to this server, how would someone besides us access the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "\n",
    "# function for generating the response of the model\n",
    "def generate(prompt: str):\n",
    "    response = ollama.chat(model=\"example\", messages=[{\"role\":\"user\", \"content\":prompt}])\n",
    "\n",
    "    return {\"response\": response[\"message\"][\"content\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web App\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjustments made to CV-Pipeline:\n",
    "1. Added function: run_and_return_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Fask App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do's:\n",
    "1. How/When create vector store for text books?\n",
    "4. **How to handle the LLM loop?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems:\n",
    "1. Requests to the LLM require loading a new state of the LLM for every request, this leads to issues with the conversion history etc <br>\n",
    "Solutions:  - store history and pass it along with a new prompt<br>\n",
    "            - Use Flask-SocketIO\n",
    "2. **DIFFERENT DEPENDENCIES FOR NUMPY**<br>\n",
    "langchain_chroma: numpy < 2.0.0, cv-pipeline: numpy = 2.2.4<br>\n",
    "We are excluding the vector store for now, but add it later with the use of different environments. \n",
    "\n",
    "3. While uploading files, the session disconnects (Maybe a Timeout?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Streamlit App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo's:\n",
    "1. Handle multiple files in a loop (Adjust part in code!)\n",
    "2. Vector doesn't work yet, due to the PDF's of the Text Books just being scans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
