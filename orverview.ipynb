{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Topics**:\n",
    "---\n",
    "1. What does the work flow for the teacher look like?\n",
    "2. How should we embed the actual exam with the corresponding \"Erwartungshorizont\" in the prompt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama (Deep Seek, LLama etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform **vector search** with [ChromaDB](https://www.trychroma.com/)<br><br>Steps\n",
    "1. Create Documents: How should this be done for the text book, what makes sense?\n",
    "2. Try new approaches for text splitting\n",
    "2. Adding id for every chunk, so we can **cite** where the LLM bases it's reasoning from\n",
    "3. The book will be stored in PDF format so we will use PyPDFLoader from langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps:\n",
    "1. How should the \n",
    "\n",
    "2. Try different Embedding functions\n",
    "3. Try different query approaches\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Ideas:\n",
    "1. First prompt should load the right exam as context for the LLM (Maybe with naming the pdfs?)\n",
    "2. First prompt also includes Background knowledge from textbook and \"Erwartungshorizont\"\n",
    "3. Precisely define the task for the LLM\n",
    "4. All prompts after should not include the Info again (-> different templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing:\n",
    "1. Use Pytest for Unit Testing\n",
    "2. Different Prompts and general approaches for Prompting\n",
    "3. Different LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to enable access to this LLM via an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages<br>\n",
    "1. fastapi\n",
    "2. uvicorn (for running the api)\n",
    "3. requests (for sending requests)\n",
    "\n",
    "How does it work with access to this server, how would someone besides us access the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "\n",
    "# function for generating the response of the model\n",
    "def generate(prompt: str):\n",
    "    response = ollama.chat(model=\"example\", messages=[{\"role\":\"user\", \"content\":prompt}])\n",
    "\n",
    "    return {\"response\": response[\"message\"][\"content\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geminin 2.5 Pro (API)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
